# 麻袋理财之反爬虫实践

    作者：王天青(Grissom) {首席架构师@麻袋理财}，褚夫元，刘文强，孙华侨
    时间：2015-12-08

我们说到爬虫，一般分为善意爬虫和恶意爬虫。善意爬虫一般来自各搜索网站，只会爬取一部分网页及信息；而恶意爬虫则会爬取整个网站的有用信息，一来会导致用户隐私及运营信息泄漏（数据是绝大多数公司的命根子），二来会加重网站服务器的负担（可以看成是一种CC攻击）。

反爬虫主要就是通过分析正常用户的访问行为及爬虫的访问行为，通过技术的手段来进行屏蔽。

本文简单分析了麻袋理财技术团队在反爬虫上做的一些尝试。

## 提纲

* 麻袋理财介绍
* 网站架构介绍
* ELK日志架构
* 正常访问行为分析
* 异常访问行为分析
* 总结

## 麻袋理财介绍

* 麻袋理财一号员工Bruce的回答：[麻袋理财怎么样?](http://www.zhihu.com/question/32165390)
* Grissom的回答：[上海及周边有哪些值得去实习的IT初创？](http://www.zhihu.com/question/29695061/answer/69698545)


## 网站架构介绍

[麻袋理财](https://www.madailicai.com)采用典型的互联网大型网站的架构

* 客户端有app, wap和PC browser
* 前后端分离，后端提供REST风格的API

说到这里，也有必要界定一下麻袋理财网站上的关键信息：

1. 单一理财产品信息：包括借款人基本信息，借款金额，借款期限，借款利率，投资人信息等。
2. 优定存投资人投资信息：包括用户ID，投资时间，投资金额等。
3. 理财产品转让信息：包括借款人基本信息，借款金额，借款期限，借款利率，已还款情况，投资人信息，转让时间等。

以上数据既包含客户的隐私信息，又能让他人统计出麻袋理财平台的运营数据。

### ELK架构
麻袋理财目前使用的ELK架构为：

    App/Service -> Logstash -> Redis -> LogStash -> Elastic -> Kibana

Redis用于做日志缓冲，第二个logStash用于写日志到Elastic。

麻袋理财使用了Nginx，我们配置的特定的Nginx格式，其中包括如下字段：

1. timestamp
2. client_ip
3. status
4. uri
5. param
6. method
7. response_time
8. http_referer
9. user_agent

这些字段就不详细解释了，不懂的同学请参看一下《HTTP权威指南》。

通常情况下我们可以使用client_ip来标识一个客户，但是考虑到很多用户可能在一个内网，外网出口是一个IP的情况，可以使用Cookie来标识客户。

### 正常访问行为分析

访问行为一般需要分场景定义，例如：

1. 未注册用户使用App或者访问网站；
2. 注册用户登陆网站后，使用App或者网站浏览投资项目；
3. 注册用户进行投资；
4. 注册用户每日查看收益；
5. 注册用户由活动页面引导进行投资等；

如果从时间维度上来看用户行为：

1. 大部分用户每日会登陆查看收益（时间一般会比较固定，因为会养成习惯）；
2. 用户偶尔会进行投资（每周或者每月一次），投资之前会浏览标的情况；
3. 用户会由活动页面引导过来进行浏览或者投资；

当然这里的用户行为分析也可以用来进行用户画像，而不是仅仅进行反爬虫。

### 异常访问行为分析

异常行为就比较多了:

1. user_agent不是App或者浏览器。例如使用了Scrapy或者HttpClient，这个比较初级，一般都会修改User Agent。
2. 短时间内大量访问。人的访问节奏一秒中能够点击5次就不得了了（游戏高手能有300到400的点击率就是旷世高手了）。
3. 顺序访问相关信息。比如顺序访问理财产品的信息，包括投资人，还款计划等；因为程序一般使用循环，而人会跳跃。
4. 定时访问。定时有可能是每日，也可能是每小时，因为爬虫是定时任务，因此比较有规律。

我们在反爬虫的过程中，发现了一个很有意思的方式（欢迎打赏），屡试不爽：

1. 过去若干时间内独立用户按照访问请求数(count)进行排序；
2. 排序的方法是该用户访问不同url的次数(URL unique count)；
3. 如果某个独立用户的访问请求数是它前面或者后面独立用户访问请求数的若干分之一或者几倍（阈值需要根据自己网站定义），那么该独立用户就很有可能是爬虫。

这里就主要利用了异常行为的第三点(遍历大量URL，而一般用户只会访问少数链接)进行，用ELK能够很容易实现。

以上的异常行为更多是统计分析，还有一些可以通过数据挖掘的方式进行分析。例如找一些用户使用我们的网站或者App，得到用户访问的日志，这样我们就能够构建用户访问URL的关联规则或者图（因为网页也好，应用也好，用户总是在打开保护B页面链接的A页面后才能访问B页面，因此存在关联关系）。

当然还有其他很多数据挖掘的方法，有待进一步挖掘。

### 总结

为了保护用户的隐私数据及网站的运营数据，我们需要：

1. 做好自身网站的建设，做好数据的访问权限控制；
2. 敏感信息需要脱敏；
3. 一旦遇到频繁访问，则需要使用验证码等方式区分人还是爬虫；
4. 使用实时计算的方式，及时发现爬虫并进行封杀（这里留个关子，下次再来讲）；

此文更多是我们所做工作的一个高度总结，其中有很多细节没有展开详解(本人也就花了一个小时一气呵成)。

如果有任何问题，欢迎联系我们(wangtianqing01@chinatopcredit.com)。
